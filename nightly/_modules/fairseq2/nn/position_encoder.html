<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>fairseq2.nn.position_encoder &#8212; fairseq2 0.3.0.dev202411250149+ga96a146 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=f6bd8450" />
    <script src="../../../_static/documentation_options.js?v=32877524"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for fairseq2.nn.position_encoder</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">final</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">embedding</span>
<span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">override</span>

<span class="kn">from</span> <span class="nn">fairseq2.nn.incremental_state</span> <span class="kn">import</span> <span class="n">IncrementalStateBag</span>
<span class="kn">from</span> <span class="nn">fairseq2.nn.padding</span> <span class="kn">import</span> <span class="n">PaddingMask</span>
<span class="kn">from</span> <span class="nn">fairseq2.typing</span> <span class="kn">import</span> <span class="n">DataType</span><span class="p">,</span> <span class="n">Device</span>


<div class="viewcode-block" id="PositionEncoder">
<a class="viewcode-back" href="../../../reference/fairseq2.nn/position_encoders.html#fairseq2.nn.PositionEncoder">[docs]</a>
<span class="k">class</span> <span class="nc">PositionEncoder</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Encodes sequences with positional information.&quot;&quot;&quot;</span>

    <span class="n">encoding_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param encoding_dim: The dimensionality of positional encodings. Input</span>
<span class="sd">            sequences are expected to have the same dimensionality.</span>
<span class="sd">        :param max_seq_len: The maximum allowed length for input sequences.</span>
<span class="sd">            Sequences longer than ``max_seq_len`` will cause a :class:`ValueError`.</span>
<span class="sd">            Typically it is set to the context length of the underlying model.</span>
<span class="sd">            If ``None``, sequences can have arbitrary length.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoding_dim</span> <span class="o">=</span> <span class="n">encoding_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>

<div class="viewcode-block" id="PositionEncoder.forward">
<a class="viewcode-back" href="../../../reference/fairseq2.nn/position_encoders.html#fairseq2.nn.PositionEncoder.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">PaddingMask</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">state_bag</span><span class="p">:</span> <span class="n">IncrementalStateBag</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a copy of ``seqs`` with positional information encoded.</span>

<span class="sd">        :param seqs: The input sequences to encode. *Shape:* :math:`(*,S,E)`,</span>
<span class="sd">            where :math:`*` is any number of batch dimensions including none,</span>
<span class="sd">            :math:`S` is the sequence length, and :math:`E` is the dimensionality</span>
<span class="sd">            of the positional encodings.</span>
<span class="sd">        :param padding_mask: The padding mask of ``seqs``. *Shape:* :math:`(*,S)`,</span>
<span class="sd">            where :math:`*` is any number of batch dimensions including none and</span>
<span class="sd">            :math:`S` is the sequence length.</span>
<span class="sd">        :param state_bag: If not ``None``, the encoder will operate in</span>
<span class="sd">            incremental decoding mode. This means that the first step in ``seqs``</span>
<span class="sd">            will be considered to be at position :attr:`state_bag.step_nr</span>
<span class="sd">            &lt;fairseq2.nn.IncrementalStateBag.step_nr&gt;` instead of 0.</span>

<span class="sd">        :raises ValueError: when the sequence length of ``seqs`` exceeds</span>
<span class="sd">            :attr:`max_seq_len`.</span>

<span class="sd">        :returns: The input sequences with positional information encoded.</span>
<span class="sd">            *Shape:* Same as ``seqs``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">or</span> <span class="n">state_bag</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">start_step</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">start_step</span> <span class="o">=</span> <span class="n">state_bag</span><span class="o">.</span><span class="n">step_nr</span>

            <span class="k">if</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">:=</span> <span class="n">start_step</span> <span class="o">+</span> <span class="n">seqs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The input sequence length must be less than or equal to the maximum sequence length (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="si">}</span><span class="s2">), but is </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_do_forward</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">,</span> <span class="n">state_bag</span><span class="p">)</span></div>


<div class="viewcode-block" id="PositionEncoder._do_forward">
<a class="viewcode-back" href="../../../reference/fairseq2.nn/position_encoders.html#fairseq2.nn.PositionEncoder._do_forward">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_do_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">PaddingMask</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">state_bag</span><span class="p">:</span> <span class="n">IncrementalStateBag</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        When overriden in a subclass, returns a copy of ``seqs`` with positional</span>
<span class="sd">        information encoded. See :meth:`forward` for parameter descriptions.</span>

<span class="sd">        :meta public:</span>
<span class="sd">        &quot;&quot;&quot;</span></div>


    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;:meta private:&quot;&quot;&quot;</span>
        <span class="n">s</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;encoding_dim=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">encoding_dim</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">, max_seq_len=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="k">return</span> <span class="n">s</span></div>



<div class="viewcode-block" id="SinusoidalPositionEncoder">
<a class="viewcode-back" href="../../../reference/fairseq2.nn/position_encoders.html#fairseq2.nn.SinusoidalPositionEncoder">[docs]</a>
<span class="nd">@final</span>
<span class="k">class</span> <span class="nc">SinusoidalPositionEncoder</span><span class="p">(</span><span class="n">PositionEncoder</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Encodes sequences with fixed sinusoidal positional information.&quot;&quot;&quot;</span>

    <span class="n">freqs</span><span class="p">:</span> <span class="n">Tensor</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">_legacy_pad_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param encoding_dim: The dimensionality of positional encodings. Input</span>
<span class="sd">            sequences are expected to have the same dimensionality.</span>
<span class="sd">        :param max_seq_len: The maximum allowed length for input sequences.</span>
<span class="sd">            Sequences longer than ``max_seq_len`` will cause a :class:`ValueError`.</span>

<span class="sd">        :raise ValueError: when ``encoding_dim`` is not even.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">encoding_dim</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoding_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`encoding_dim` must be even, but is </span><span class="si">{</span><span class="n">encoding_dim</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># This is a legacy parameter that should only be set when the encodings</span>
        <span class="c1"># must be compatible with fairseq.</span>
        <span class="k">if</span> <span class="n">_legacy_pad_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sin_offset</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sin_offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">_legacy_pad_idx</span>

        <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">encoding_dim</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;freqs&quot;</span><span class="p">,</span> <span class="n">freqs</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

<div class="viewcode-block" id="SinusoidalPositionEncoder.reset_parameters">
<a class="viewcode-back" href="../../../reference/fairseq2.nn/position_encoders.html#fairseq2.nn.SinusoidalPositionEncoder.reset_parameters">[docs]</a>
    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the parameters and buffers of the module.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_non_persistent_buffers</span><span class="p">()</span></div>


<div class="viewcode-block" id="SinusoidalPositionEncoder.reset_non_persistent_buffers">
<a class="viewcode-back" href="../../../reference/fairseq2.nn/position_encoders.html#fairseq2.nn.SinusoidalPositionEncoder.reset_non_persistent_buffers">[docs]</a>
    <span class="k">def</span> <span class="nf">reset_non_persistent_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the non-persistent buffers of the module.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span><span class="o">.</span><span class="n">dtype</span>

        <span class="n">num_sin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoding_dim</span> <span class="o">//</span> <span class="mi">2</span>

        <span class="n">l_half</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_sin</span><span class="p">]</span>
        <span class="n">r_half</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span><span class="p">[:,</span> <span class="n">num_sin</span><span class="p">:]</span>

        <span class="n">start_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sin_offset</span>

        <span class="c1"># (S)</span>
        <span class="n">steps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="n">start_step</span><span class="p">,</span> <span class="n">start_step</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
        <span class="p">)</span>

        <span class="c1"># (E)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_sin</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># This is identical to tensor2tensor&#39;s implementation.</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">indices</span> <span class="o">*</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_sin</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># (S) x (E) -&gt; (S, E)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">freqs</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">l_half</span><span class="p">)</span>

        <span class="n">r_half</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">l_half</span><span class="p">)</span>

        <span class="n">l_half</span><span class="o">.</span><span class="n">sin_</span><span class="p">()</span>
        <span class="n">r_half</span><span class="o">.</span><span class="n">cos_</span><span class="p">()</span></div>


    <span class="nd">@override</span>
    <span class="k">def</span> <span class="nf">_do_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">PaddingMask</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">state_bag</span><span class="p">:</span> <span class="n">IncrementalStateBag</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;:meta private:&quot;&quot;&quot;</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">seqs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">or</span> <span class="n">state_bag</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">start_step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">start_step</span> <span class="o">=</span> <span class="n">state_bag</span><span class="o">.</span><span class="n">step_nr</span>

        <span class="n">fp32_seqs</span> <span class="o">=</span> <span class="n">seqs</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span><span class="p">[</span><span class="n">start_step</span> <span class="p">:</span> <span class="n">start_step</span> <span class="o">+</span> <span class="n">seq_len</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">fp32_seqs</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">seqs</span><span class="p">)</span></div>



<div class="viewcode-block" id="LearnedPositionEncoder">
<a class="viewcode-back" href="../../../reference/fairseq2.nn/position_encoders.html#fairseq2.nn.LearnedPositionEncoder">[docs]</a>
<span class="nd">@final</span>
<span class="k">class</span> <span class="nc">LearnedPositionEncoder</span><span class="p">(</span><span class="n">PositionEncoder</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Encodes sequences with learned positional embeddings.&quot;&quot;&quot;</span>

    <span class="n">weight</span><span class="p">:</span> <span class="n">Parameter</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">DataType</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param encoding_dim: The dimensionality of positional encodings. Input</span>
<span class="sd">            sequences are expected to have the same dimensionality.</span>
<span class="sd">        :param max_seq_len: The maximum allowed length for input sequences.</span>
<span class="sd">            Sequences longer than ``max_seq_len`` will cause a :class:`ValueError`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">encoding_dim</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">encoding_dim</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

<div class="viewcode-block" id="LearnedPositionEncoder.reset_parameters">
<a class="viewcode-back" href="../../../reference/fairseq2.nn/position_encoders.html#fairseq2.nn.LearnedPositionEncoder.reset_parameters">[docs]</a>
    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the parameters and buffers of the module.&quot;&quot;&quot;</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span></div>


    <span class="nd">@override</span>
    <span class="k">def</span> <span class="nf">_do_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">PaddingMask</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">state_bag</span><span class="p">:</span> <span class="n">IncrementalStateBag</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;:meta private:&quot;&quot;&quot;</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">seqs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">or</span> <span class="n">state_bag</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">start_step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">start_step</span> <span class="o">=</span> <span class="n">state_bag</span><span class="o">.</span><span class="n">step_nr</span>

        <span class="n">steps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="n">start_step</span><span class="p">,</span> <span class="n">start_step</span> <span class="o">+</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">seqs</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">seqs</span> <span class="o">+</span> <span class="n">embedding</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span></div>



<div class="viewcode-block" id="RotaryEncoder">
<a class="viewcode-back" href="../../../reference/fairseq2.nn/position_encoders.html#fairseq2.nn.RotaryEncoder">[docs]</a>
<span class="nd">@final</span>
<span class="k">class</span> <span class="nc">RotaryEncoder</span><span class="p">(</span><span class="n">PositionEncoder</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes sequences with relative positional information as described in</span>
<span class="sd">    :cite:t:`https://doi.org/10.48550/arxiv.2104.09864`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">freqs</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">theta</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">freqs_init_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">RotaryEncoder</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10_000.0</span><span class="p">,</span>
        <span class="n">freqs_init_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">RotaryEncoder</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param encoding_dim: The dimensionality of positional encodings. Input</span>
<span class="sd">            sequences are expected to have the same dimensionality.</span>
<span class="sd">        :param max_seq_len: The maximum allowed length for input sequences.</span>
<span class="sd">            Sequences longer than ``max_seq_len`` will cause a :class:`ValueError`.</span>
<span class="sd">        :param theta: The coefficient of the long-term decay as described in</span>
<span class="sd">            section 3.3 of the reference paper.</span>
<span class="sd">        :param freqs_init_fn: A callable to initialize the frequency table. The</span>
<span class="sd">            encoder will be passed to the callable as an argument and it is</span>
<span class="sd">            expected for the callable to return a :class:`~torch.Tensor` holding</span>
<span class="sd">            the frequency table. If ``None``, the frequencies will be initialized</span>
<span class="sd">            as described in the reference paper.</span>

<span class="sd">        :raise ValueError: when ``encoding_dim`` is not even.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">encoding_dim</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoding_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`encoding_dim` must be even, but is </span><span class="si">{</span><span class="n">encoding_dim</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
            <span class="p">)</span>

        <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">encoding_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;freqs&quot;</span><span class="p">,</span> <span class="n">freqs</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">freqs_init_fn</span> <span class="o">=</span> <span class="n">freqs_init_fn</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

<div class="viewcode-block" id="RotaryEncoder.reset_parameters">
<a class="viewcode-back" href="../../../reference/fairseq2.nn/position_encoders.html#fairseq2.nn.RotaryEncoder.reset_parameters">[docs]</a>
    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the parameters and buffers of the module.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_non_persistent_buffers</span><span class="p">()</span></div>


<div class="viewcode-block" id="RotaryEncoder.reset_non_persistent_buffers">
<a class="viewcode-back" href="../../../reference/fairseq2.nn/position_encoders.html#fairseq2.nn.RotaryEncoder.reset_non_persistent_buffers">[docs]</a>
    <span class="k">def</span> <span class="nf">reset_non_persistent_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the non-persistent buffers of the module.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span><span class="o">.</span><span class="n">device</span>

        <span class="n">complex_freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">freqs</span><span class="p">)</span>

        <span class="c1"># (S)</span>
        <span class="n">steps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">freqs_init_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># (E / 2)</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoding_dim</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
            <span class="p">)</span>

            <span class="n">freqs</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">**</span> <span class="p">(</span><span class="n">indices</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoding_dim</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">freqs_init_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="c1"># (S) x (E / 2) -&gt; (S, E / 2)</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>

        <span class="c1"># (S, E / 2)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">polar</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">freqs</span><span class="p">),</span> <span class="n">freqs</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">complex_freqs</span><span class="p">)</span></div>


    <span class="nd">@override</span>
    <span class="k">def</span> <span class="nf">_do_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seqs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_mask</span><span class="p">:</span> <span class="n">PaddingMask</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">state_bag</span><span class="p">:</span> <span class="n">IncrementalStateBag</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;:meta private:&quot;&quot;&quot;</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">seqs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">or</span> <span class="n">state_bag</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">start_step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">start_step</span> <span class="o">=</span> <span class="n">state_bag</span><span class="o">.</span><span class="n">step_nr</span>

        <span class="n">complex_freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">freqs</span><span class="p">)</span>

        <span class="n">complex_freqs</span> <span class="o">=</span> <span class="n">complex_freqs</span><span class="p">[</span><span class="n">start_step</span> <span class="p">:</span> <span class="n">start_step</span> <span class="o">+</span> <span class="n">seq_len</span><span class="p">]</span>

        <span class="c1"># (*, S, E) -&gt; (*, S, E / 2, 2)</span>
        <span class="n">seqs</span> <span class="o">=</span> <span class="n">seqs</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="n">complex_seqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">seqs</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>

        <span class="n">complex_seqs</span> <span class="o">=</span> <span class="n">complex_seqs</span> <span class="o">*</span> <span class="n">complex_freqs</span>

        <span class="c1"># (*, S, E / 2, 2) -&gt; (*, S, E)</span>
        <span class="n">fp32_seqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">complex_seqs</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fp32_seqs</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">seqs</span><span class="p">)</span></div>

</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">fairseq2</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../topics/index.html">Using fairseq2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/index.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bibliography.html">Bibliography</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      
      
      
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.4.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
    </div>

    

    
  </body>
</html>