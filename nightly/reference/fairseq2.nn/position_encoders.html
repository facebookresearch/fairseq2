<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Position Encoders &#8212; fairseq2 0.3.0.dev202410210149+g822bc1a documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css?v=f6bd8450" />
    <script src="../../_static/documentation_options.js?v=f9deffac"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Bibliography" href="../../bibliography.html" />
    <link rel="prev" title="fairseq2.nn" href="index.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="position-encoders">
<h1>Position Encoders<a class="headerlink" href="#position-encoders" title="Link to this heading">¶</a></h1>
<p>A set of PyTorch modules to encode sequences with positional information.</p>
<p><strong>ABCs</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="#fairseq2.nn.PositionEncoder" title="fairseq2.nn.PositionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">PositionEncoder</span></code></a></p></li>
</ul>
<p><strong>Classes</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="#fairseq2.nn.SinusoidalPositionEncoder" title="fairseq2.nn.SinusoidalPositionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">SinusoidalPositionEncoder</span></code></a></p></li>
<li><p><a class="reference internal" href="#fairseq2.nn.LearnedPositionEncoder" title="fairseq2.nn.LearnedPositionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">LearnedPositionEncoder</span></code></a></p></li>
<li><p><a class="reference internal" href="#fairseq2.nn.RotaryEncoder" title="fairseq2.nn.RotaryEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">RotaryEncoder</span></code></a></p></li>
</ul>
<section id="abcs">
<h2>ABCs<a class="headerlink" href="#abcs" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="fairseq2.nn.PositionEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">fairseq2.nn.</span></span><span class="sig-name descname"><span class="pre">PositionEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoding_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_seq_len</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/fairseq2/nn/position_encoder.html#PositionEncoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.nn.PositionEncoder" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/abc.html#abc.ABC" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ABC</span></code></a></p>
<p>Encodes sequences with positional information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The dimensionality of positional encodings. Input
sequences are expected to have the same dimensionality.</p></li>
<li><p><strong>max_seq_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> | </em><em>None</em>) – The maximum allowed length for input sequences.
Sequences longer than <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> will cause a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a>.
Typically it is set to the context length of the underlying model.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, sequences can have arbitrary length.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.nn.PositionEncoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seqs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mask</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_bag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/fairseq2/nn/position_encoder.html#PositionEncoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.nn.PositionEncoder.forward" title="Link to this definition">¶</a></dt>
<dd><p>Returns a copy of <code class="docutils literal notranslate"><span class="pre">seqs</span></code> with positional information encoded.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seqs</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><em>Tensor</em></a>) – The input sequences to encode. <em>Shape:</em> <span class="math notranslate nohighlight">\((*,S,E)\)</span>,
where <span class="math notranslate nohighlight">\(*\)</span> is any number of batch dimensions including none,
<span class="math notranslate nohighlight">\(S\)</span> is the sequence length, and <span class="math notranslate nohighlight">\(E\)</span> is the dimensionality
of the positional encodings.</p></li>
<li><p><strong>padding_mask</strong> (<em>PaddingMask</em><em> | </em><em>None</em>) – The padding mask of <code class="docutils literal notranslate"><span class="pre">seqs</span></code>. <em>Shape:</em> <span class="math notranslate nohighlight">\((*,S)\)</span>,
where <span class="math notranslate nohighlight">\(*\)</span> is any number of batch dimensions including none and
<span class="math notranslate nohighlight">\(S\)</span> is the sequence length.</p></li>
<li><p><strong>state_bag</strong> (<em>IncrementalStateBag</em><em> | </em><em>None</em>) – If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, the encoder will operate in
incremental decoding mode. This means that the first step in <code class="docutils literal notranslate"><span class="pre">seqs</span></code>
will be considered to be at position <code class="xref py py-attr docutils literal notranslate"><span class="pre">state_bag.step_nr</span></code> instead of 0.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – when the sequence length of <code class="docutils literal notranslate"><span class="pre">seqs</span></code> exceeds
<code class="xref py py-attr docutils literal notranslate"><span class="pre">max_seq_len</span></code>.</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The input sequences with positional information encoded.
<em>Shape:</em> Same as <code class="docutils literal notranslate"><span class="pre">seqs</span></code>.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.nn.PositionEncoder._do_forward">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_do_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seqs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_bag</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/fairseq2/nn/position_encoder.html#PositionEncoder._do_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.nn.PositionEncoder._do_forward" title="Link to this definition">¶</a></dt>
<dd><p>When overriden in a subclass, returns a copy of <code class="docutils literal notranslate"><span class="pre">seqs</span></code> with positional
information encoded. See <a class="reference internal" href="#fairseq2.nn.PositionEncoder.forward" title="fairseq2.nn.PositionEncoder.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for parameter descriptions.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><em>Tensor</em></a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="classes">
<h2>Classes<a class="headerlink" href="#classes" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="fairseq2.nn.SinusoidalPositionEncoder">
<em class="property"><span class="pre">final</span><span class="w"> </span><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">fairseq2.nn.</span></span><span class="sig-name descname"><span class="pre">SinusoidalPositionEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoding_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_seq_len</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_legacy_pad_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/fairseq2/nn/position_encoder.html#SinusoidalPositionEncoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.nn.SinusoidalPositionEncoder" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#fairseq2.nn.PositionEncoder" title="fairseq2.nn.position_encoder.PositionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">PositionEncoder</span></code></a></p>
<p>Encodes sequences with fixed sinusoidal positional information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The dimensionality of positional encodings. Input
sequences are expected to have the same dimensionality.</p></li>
<li><p><strong>max_seq_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> | </em><em>None</em>) – The maximum allowed length for input sequences.
Sequences longer than <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> will cause a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – when <code class="docutils literal notranslate"><span class="pre">encoding_dim</span></code> is not even.</p>
</dd>
</dl>
<p>The positional encodings are initialized as in tensor2tensor which differs
slightly from the description in section 3.5 of
<span id="id1">Vaswani <em>et al.</em> [<a class="reference internal" href="../../bibliography.html#id5" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017. URL: https://arxiv.org/abs/1706.03762, doi:10.48550/ARXIV.1706.03762.">VSP+17</a>]</span>. This means instead of:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}PE_{(pos, 2i)}   = \text{sin}(pos/10000^{2i/d_{model}})\\PE_{(pos, 2i+1)} = \text{cos}(pos/10000^{2i/d_{model}})\end{aligned}\end{align} \]</div>
<p>we use:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}PE_{(pos, i)} = \text{sin}(pos/10000^{i/d_{model}})\;\text{for}\;i\;   &lt;\frac{d_{model}}{2}\\PE_{(pos, i)} = \text{cos}(pos/10000^{i/d_{model}})\;\text{for}\;i\;\geq\frac{d_{model}}{2}\end{aligned}\end{align} \]</div>
<p>See <a class="reference external" href="https://github.com/tensorflow/tensor2tensor/pull/177">here</a> for more
information.</p>
<p>Usage:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">fairseq2.nn.position_encoder</span> <span class="kn">import</span> <span class="n">SinusoidalPositionEncoder</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">SinusoidalPositionEncoder</span><span class="p">(</span><span class="n">encoding_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">seqs</span><span class="p">)</span>
<span class="go">tensor([[ 1.0000e+00,  1.0000e+00,  2.0000e+00,  2.0000e+00],  # pos 0</span>
<span class="go">        [ 9.4147e-01,  2.0000e-04,  6.4030e-01,  2.0000e+00],  # pos 1</span>
<span class="go">        [ 1.0930e-02,  3.0000e-04, -5.1615e-01,  2.0000e+00]]) # pos 2</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.nn.SinusoidalPositionEncoder.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/fairseq2/nn/position_encoder.html#SinusoidalPositionEncoder.reset_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.nn.SinusoidalPositionEncoder.reset_parameters" title="Link to this definition">¶</a></dt>
<dd><p>Reset the parameters and buffers of the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.nn.SinusoidalPositionEncoder.reset_non_persistent_buffers">
<span class="sig-name descname"><span class="pre">reset_non_persistent_buffers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/fairseq2/nn/position_encoder.html#SinusoidalPositionEncoder.reset_non_persistent_buffers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.nn.SinusoidalPositionEncoder.reset_non_persistent_buffers" title="Link to this definition">¶</a></dt>
<dd><p>Reset the non-persistent buffers of the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="fairseq2.nn.LearnedPositionEncoder">
<em class="property"><span class="pre">final</span><span class="w"> </span><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">fairseq2.nn.</span></span><span class="sig-name descname"><span class="pre">LearnedPositionEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoding_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_seq_len</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/fairseq2/nn/position_encoder.html#LearnedPositionEncoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.nn.LearnedPositionEncoder" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#fairseq2.nn.PositionEncoder" title="fairseq2.nn.position_encoder.PositionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">PositionEncoder</span></code></a></p>
<p>Encodes sequences with learned positional embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The dimensionality of positional encodings. Input
sequences are expected to have the same dimensionality.</p></li>
<li><p><strong>max_seq_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> | </em><em>None</em>) – The maximum allowed length for input sequences.
Sequences longer than <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> will cause a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<p>Usage:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">fairseq2.nn.position_encoder</span> <span class="kn">import</span> <span class="n">LearnedPositionEncoder</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">LearnedPositionEncoder</span><span class="p">(</span><span class="n">encoding_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">seqs</span><span class="p">)</span>
<span class="go">tensor([[ 1.1135,  0.5548,  0.4293,  2.0112],                               # pos 0</span>
<span class="go">        [ 0.2364,  0.6009,  3.3865, -2.4810],                               # pos 1</span>
<span class="go">        [-0.4746,  0.4544,  0.2761,  0.8828]], grad_fn=&lt;SqueezeBackward1&gt;)  # pos 2</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.nn.LearnedPositionEncoder.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/fairseq2/nn/position_encoder.html#LearnedPositionEncoder.reset_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.nn.LearnedPositionEncoder.reset_parameters" title="Link to this definition">¶</a></dt>
<dd><p>Reset the parameters and buffers of the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="fairseq2.nn.RotaryEncoder">
<em class="property"><span class="pre">final</span><span class="w"> </span><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">fairseq2.nn.</span></span><span class="sig-name descname"><span class="pre">RotaryEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoding_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_seq_len</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">theta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freqs_init_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/fairseq2/nn/position_encoder.html#RotaryEncoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.nn.RotaryEncoder" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#fairseq2.nn.PositionEncoder" title="fairseq2.nn.position_encoder.PositionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">PositionEncoder</span></code></a></p>
<p>Encodes sequences with relative positional information as described in
<span id="id2">Su <em>et al.</em> [<a class="reference internal" href="../../bibliography.html#id19" title="Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. 2021. URL: https://arxiv.org/abs/2104.09864, doi:10.48550/ARXIV.2104.09864.">SLP+21</a>]</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The dimensionality of positional encodings. Input
sequences are expected to have the same dimensionality.</p></li>
<li><p><strong>max_seq_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> | </em><em>None</em>) – The maximum allowed length for input sequences.
Sequences longer than <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> will cause a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a>.</p></li>
<li><p><strong>theta</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – The coefficient of the long-term decay as described in
section 3.3 of the reference paper.</p></li>
<li><p><strong>freqs_init_fn</strong> (<a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable" title="(in Python v3.13)"><em>Callable</em></a><em>[</em><em>[</em><a class="reference internal" href="#fairseq2.nn.RotaryEncoder" title="fairseq2.nn.position_encoder.RotaryEncoder"><em>RotaryEncoder</em></a><em>]</em><em>, </em><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><em>Tensor</em></a><em>] </em><em>| </em><em>None</em>) – A callable to initialize the frequency table. The
encoder will be passed to the callable as an argument and it is
expected for the callable to return a <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> holding
the frequency table. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the frequencies will be initialized
as described in the reference paper.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – when <code class="docutils literal notranslate"><span class="pre">encoding_dim</span></code> is not even.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.nn.RotaryEncoder.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/fairseq2/nn/position_encoder.html#RotaryEncoder.reset_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.nn.RotaryEncoder.reset_parameters" title="Link to this definition">¶</a></dt>
<dd><p>Reset the parameters and buffers of the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.nn.RotaryEncoder.reset_non_persistent_buffers">
<span class="sig-name descname"><span class="pre">reset_non_persistent_buffers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/fairseq2/nn/position_encoder.html#RotaryEncoder.reset_non_persistent_buffers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.nn.RotaryEncoder.reset_non_persistent_buffers" title="Link to this definition">¶</a></dt>
<dd><p>Reset the non-persistent buffers of the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">fairseq2</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../topics/index.html">Using fairseq2</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../fairseq2.html">fairseq2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../fairseq2.data.text/index.html">fairseq2.data.text</a></li>
<li class="toctree-l2"><a class="reference internal" href="../fairseq2.dependency.html">fairseq2.dependency</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">fairseq2.nn</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">API Reference</a><ul>
  <li><a href="index.html">fairseq2.nn</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">fairseq2.nn</a></li>
      <li>Next: <a href="../../bibliography.html" title="next chapter">Bibliography</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      
      
      
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.4.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="../../_sources/reference/fairseq2.nn/position_encoders.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>