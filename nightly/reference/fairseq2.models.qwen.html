<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
    <link href="../_static/img/logo.svg" rel="icon" type="image/svg+xml"><link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="fairseq2.recipe.composition" href="fairseq2.recipe.composition.html" /><link rel="prev" title="fairseq2.models.llama" href="fairseq2.models.llama.html" />

    <!-- Generated with Sphinx 7.4.7 and Furo 2024.08.06 -->
        <title>fairseq2.models.qwen - fairseq2 Documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --color-brand-primary: #008080;
  --color-brand-content: #008080;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">fairseq2 Documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.svg" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">fairseq2 Documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Latest News</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news/whats_new_v0_5.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-report" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M1.75 1.5a.25.25 0 00-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 01.75.75v2.19l2.72-2.72a.75.75 0 01.53-.22h6.5a.25.25 0 00.25-.25v-9.5a.25.25 0 00-.25-.25H1.75zM0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0114.25 13H8.06l-2.573 2.573A1.457 1.457 0 013 14.543V13H1.75A1.75 1.75 0 010 11.25v-9.5zM9 9a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z" fill-rule="evenodd"></path></svg> What’s New in v0.5</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../getting_started/installation/index.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-download" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M7.47 10.78a.75.75 0 001.06 0l3.75-3.75a.75.75 0 00-1.06-1.06L8.75 8.44V1.75a.75.75 0 00-1.5 0v6.69L4.78 5.97a.75.75 0 00-1.06 1.06l3.75 3.75zM3.75 13a.75.75 0 000 1.5h8.5a.75.75 0 000-1.5h-8.5z" fill-rule="evenodd"></path></svg> Installation</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of  Installation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/installation/installation_from_source.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-file-binary" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2a.75.75 0 01-1.5 0v-2zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM0 7.75C0 6.784.784 6 1.75 6h1.5C4.216 6 5 6.784 5 7.75v2.5A1.75 1.75 0 013.25 12h-1.5A1.75 1.75 0 010 10.25v-2.5zm1.75-.25a.25.25 0 00-.25.25v2.5c0 .138.112.25.25.25h1.5a.25.25 0 00.25-.25v-2.5a.25.25 0 00-.25-.25h-1.5zm5-1.5a.75.75 0 000 1.5h.75v3h-.75a.75.75 0 000 1.5h3a.75.75 0 000-1.5H9V6.75A.75.75 0 008.25 6h-1.5z" fill-rule="evenodd"></path></svg> Installing from Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/installation/setup_with_uv.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-lock" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4 4v2h-.25A1.75 1.75 0 002 7.75v5.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 13.25v-5.5A1.75 1.75 0 0012.25 6H12V4a4 4 0 10-8 0zm6.5 2V4a2.5 2.5 0 00-5 0v2h5zM12 7.5h.25a.25.25 0 01.25.25v5.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-5.5a.25.25 0 01.25-.25H12z" fill-rule="evenodd"></path></svg> UV Setup</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../basics/design_philosophy.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-infinity" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M3.5 6c-1.086 0-2 .914-2 2 0 1.086.914 2 2 2 .525 0 1.122-.244 1.825-.727.51-.35 1.025-.79 1.561-1.273-.536-.483-1.052-.922-1.56-1.273C4.621 6.244 4.025 6 3.5 6zm4.5.984c-.59-.533-1.204-1.066-1.825-1.493-.797-.548-1.7-.991-2.675-.991C1.586 4.5 0 6.086 0 8s1.586 3.5 3.5 3.5c.975 0 1.878-.444 2.675-.991.621-.427 1.235-.96 1.825-1.493.59.533 1.204 1.066 1.825 1.493.797.547 1.7.991 2.675.991 1.914 0 3.5-1.586 3.5-3.5s-1.586-3.5-3.5-3.5c-.975 0-1.878.443-2.675.991-.621.427-1.235.96-1.825 1.493zM9.114 8c.536.483 1.052.922 1.56 1.273.704.483 1.3.727 1.826.727 1.086 0 2-.914 2-2 0-1.086-.914-2-2-2-.525 0-1.122.244-1.825.727-.51.35-1.025.79-1.561 1.273z" fill-rule="evenodd"></path></svg> Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/assets.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-container" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M10.41.24l4.711 2.774A1.767 1.767 0 0116 4.54v5.01a1.77 1.77 0 01-.88 1.53l-7.753 4.521-.002.001a1.767 1.767 0 01-1.774 0H5.59L.873 12.85A1.762 1.762 0 010 11.327V6.292c0-.304.078-.598.22-.855l.004-.005.01-.019c.15-.262.369-.486.64-.643L8.641.239a1.75 1.75 0 011.765 0l.002.001zM9.397 1.534a.25.25 0 01.252 0l4.115 2.422-7.152 4.148a.267.267 0 01-.269 0L2.227 5.716l7.17-4.182zM7.365 9.402L8.73 8.61v4.46l-1.5.875V9.473a1.77 1.77 0 00.136-.071zm2.864 2.794V7.741l1.521-.882v4.45l-1.521.887zm3.021-1.762l1.115-.65h.002a.268.268 0 00.133-.232V5.264l-1.25.725v4.445zm-11.621 1.12l4.1 2.393V9.474a1.77 1.77 0 01-.138-.072L1.5 7.029v4.298c0 .095.05.181.129.227z" fill-rule="evenodd"></path></svg> Assets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/building_recipes.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-rocket" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M14.064 0a8.75 8.75 0 00-6.187 2.563l-.459.458c-.314.314-.616.641-.904.979H3.31a1.75 1.75 0 00-1.49.833L.11 7.607a.75.75 0 00.418 1.11l3.102.954c.037.051.079.1.124.145l2.429 2.428c.046.046.094.088.145.125l.954 3.102a.75.75 0 001.11.418l2.774-1.707a1.75 1.75 0 00.833-1.49V9.485c.338-.288.665-.59.979-.904l.458-.459A8.75 8.75 0 0016 1.936V1.75A1.75 1.75 0 0014.25 0h-.186zM10.5 10.625c-.088.06-.177.118-.266.175l-2.35 1.521.548 1.783 1.949-1.2a.25.25 0 00.119-.213v-2.066zM3.678 8.116L5.2 5.766c.058-.09.117-.178.176-.266H3.309a.25.25 0 00-.213.119l-1.2 1.95 1.782.547zm5.26-4.493A7.25 7.25 0 0114.063 1.5h.186a.25.25 0 01.25.25v.186a7.25 7.25 0 01-2.123 5.127l-.459.458a15.21 15.21 0 01-2.499 2.02l-2.317 1.5-2.143-2.143 1.5-2.317a15.25 15.25 0 012.02-2.5l.458-.458h.002zM12 5a1 1 0 11-2 0 1 1 0 012 0zm-8.44 9.56a1.5 1.5 0 10-2.12-2.12c-.734.73-1.047 2.332-1.15 3.003a.23.23 0 00.265.265c.671-.103 2.273-.416 3.005-1.148z" fill-rule="evenodd"></path></svg> Building Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/runtime_extension.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-plug" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M10.276 3.09a.25.25 0 01.192-.09h.782a.25.25 0 01.25.25v8.5a.25.25 0 01-.25.25h-.782a.25.25 0 01-.192-.09l-.95-1.14a.75.75 0 00-.483-.264l-3.124-.39a.25.25 0 01-.219-.249V5.133a.25.25 0 01.219-.248l3.124-.39a.75.75 0 00.483-.265l.95-1.14zM4 8v1.867a1.75 1.75 0 001.533 1.737l2.83.354.761.912c.332.4.825.63 1.344.63h.782A1.75 1.75 0 0013 11.75V11h2.25a.75.75 0 000-1.5H13v-4h2.25a.75.75 0 000-1.5H13v-.75a1.75 1.75 0 00-1.75-1.75h-.782c-.519 0-1.012.23-1.344.63l-.76.913-2.831.353A1.75 1.75 0 004 5.133V6.5H2.5A2.5 2.5 0 000 9v5.25a.75.75 0 001.5 0V9a1 1 0 011-1H4z" fill-rule="evenodd"></path></svg> Runtime Extension</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guides/add_model.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-ruby" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M3.637 2.291A.75.75 0 014.23 2h7.54a.75.75 0 01.593.291l3.48 4.5a.75.75 0 01-.072.999l-7.25 7a.75.75 0 01-1.042 0l-7.25-7a.75.75 0 01-.072-.999l3.48-4.5zM4.598 3.5L1.754 7.177 8 13.207l6.246-6.03L11.402 3.5H4.598z" fill-rule="evenodd"></path></svg> Add Your Own Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guides/pudb.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-bug" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4.72.22a.75.75 0 011.06 0l1 .999a3.492 3.492 0 012.441 0l.999-1a.75.75 0 111.06 1.061l-.775.776c.616.63.995 1.493.995 2.444v.327c0 .1-.009.197-.025.292.408.14.764.392 1.029.722l1.968-.787a.75.75 0 01.556 1.392L13 7.258V9h2.25a.75.75 0 010 1.5H13v.5c0 .409-.049.806-.141 1.186l2.17.868a.75.75 0 01-.557 1.392l-2.184-.873A4.997 4.997 0 018 16a4.997 4.997 0 01-4.288-2.427l-2.183.873a.75.75 0 01-.558-1.392l2.17-.868A5.013 5.013 0 013 11v-.5H.75a.75.75 0 010-1.5H3V7.258L.971 6.446a.75.75 0 01.558-1.392l1.967.787c.265-.33.62-.583 1.03-.722a1.684 1.684 0 01-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72 1.28a.75.75 0 010-1.06zM6.173 5h3.654A.173.173 0 0010 4.827V4.5a2 2 0 10-4 0v.327c0 .096.077.173.173.173zM5.25 6.5a.75.75 0 00-.75.75V11a3.5 3.5 0 107 0V7.25a.75.75 0 00-.75-.75h-5.5z" fill-rule="evenodd"></path></svg> Debugging with PuDB</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../concepts/assets.html">Assets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/gang.html">Gangs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="assets.html">fairseq2.assets</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">fairseq2.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.data.html">fairseq2.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.data.tokenizers.html">fairseq2.data.tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.data.tokenizers.hub.html">fairseq2.data.tokenizers.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.datasets.html">fairseq2.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.datasets.hub.html">fairseq2.datasets.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.device.html">fairseq2.device</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.gang.html">fairseq2.gang</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.model_checkpoint.html">fairseq2.model_checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.models.html">fairseq2.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.models.hub.html">fairseq2.models.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.models.llama.html">fairseq2.models.llama</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">fairseq2.models.qwen</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.recipe.composition.html">fairseq2.recipe.composition</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.recipe.optim.html">fairseq2.recipe.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.utils.validation.html">fairseq2.utils.validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.nn.html">fairseq2.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.nn.batch_layout.html">fairseq2.nn.batch_layout</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.nn.data_parallel.html">fairseq2.nn.data_parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.nn.embedding.html">fairseq2.nn.embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.nn.incremental_state.html">fairseq2.nn.incremental_state</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.nn.normalization.html">fairseq2.nn.normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.nn.position_encoder.html">fairseq2.nn.position_encoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.nn.projection.html">fairseq2.nn.projection</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.nn.residual.html">fairseq2.nn.residual</a></li>
<li class="toctree-l1"><a class="reference internal" href="fairseq2.nn.utils.html">fairseq2.nn.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">fairseq2.logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">fairseq2.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">fairseq2.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipe.html">fairseq2.recipe</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../other/contributing.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-heart" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4.25 2.5c-1.336 0-2.75 1.164-2.75 3 0 2.15 1.58 4.144 3.365 5.682A20.565 20.565 0 008 13.393a20.561 20.561 0 003.135-2.211C12.92 9.644 14.5 7.65 14.5 5.5c0-1.836-1.414-3-2.75-3-1.373 0-2.609.986-3.029 2.456a.75.75 0 01-1.442 0C6.859 3.486 5.623 2.5 4.25 2.5zM8 14.25l-.345.666-.002-.001-.006-.003-.018-.01a7.643 7.643 0 01-.31-.17 22.075 22.075 0 01-3.434-2.414C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.045 5.231-3.885 6.818a22.08 22.08 0 01-3.744 2.584l-.018.01-.006.003h-.002L8 14.25zm0 0l.345.666a.752.752 0 01-.69 0L8 14.25z" fill-rule="evenodd"></path></svg> Contributing to fairseq2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../other/bibliography.html"><svg aria-hidden="true" class="sd-octicon sd-octicon-book" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M0 1.75A.75.75 0 01.75 1h4.253c1.227 0 2.317.59 3 1.501A3.744 3.744 0 0111.006 1h4.245a.75.75 0 01.75.75v10.5a.75.75 0 01-.75.75h-4.507a2.25 2.25 0 00-1.591.659l-.622.621a.75.75 0 01-1.06 0l-.622-.621A2.25 2.25 0 005.258 13H.75a.75.75 0 01-.75-.75V1.75zm8.755 3a2.25 2.25 0 012.25-2.25H14.5v9h-3.757c-.71 0-1.4.201-1.992.572l.004-7.322zm-1.504 7.324l.004-5.073-.002-2.253A2.25 2.25 0 005.003 2.5H1.5v9h3.757a3.75 3.75 0 011.994.574z" fill-rule="evenodd"></path></svg> Bibliography</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/reference/fairseq2.models.qwen.rst.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="fairseq2-models-qwen">
<span id="api-models-qwen"></span><h1>fairseq2.models.qwen<a class="headerlink" href="#fairseq2-models-qwen" title="Link to this heading">¶</a></h1>
<p>The Qwen module provides support for Qwen2.5 and Qwen3 language models.
It includes model configurations, hub access, tokenizers, and utilities for loading and working with Qwen models.</p>
<section id="quick-start">
<h2>Quick Start<a class="headerlink" href="#quick-start" title="Link to this heading">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_qwen_model_hub</span><span class="p">,</span> <span class="n">get_qwen_tokenizer_hub</span>

<span class="c1"># Get the model hub</span>
<span class="n">hub</span> <span class="o">=</span> <span class="n">get_qwen_model_hub</span><span class="p">()</span>

<span class="c1"># List available architectures</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Available Qwen architectures:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">arch</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">hub</span><span class="o">.</span><span class="n">get_archs</span><span class="p">()):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - </span><span class="si">{</span><span class="n">arch</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Load a model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;qwen25_7b&quot;</span><span class="p">)</span>

<span class="c1"># Load corresponding tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_qwen_tokenizer_hub</span><span class="p">()</span><span class="o">.</span><span class="n">load_tokenizer</span><span class="p">(</span><span class="s2">&quot;qwen25_7b&quot;</span><span class="p">)</span>

<span class="c1"># Generate some text</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;The future of AI is&quot;</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">create_encoder</span><span class="p">()</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="c1"># ... model inference code ...</span>
</pre></div>
</div>
</section>
<section id="available-models">
<h2>Available Models<a class="headerlink" href="#available-models" title="Link to this heading">¶</a></h2>
<p>The Qwen family includes several model sizes and versions:</p>
<p><strong>Qwen 2.5 Series:</strong>
- <code class="docutils literal notranslate"><span class="pre">qwen25_1_5b</span></code> - 1.5B parameters
- <code class="docutils literal notranslate"><span class="pre">qwen25_3b</span></code> - 3B parameters
- <code class="docutils literal notranslate"><span class="pre">qwen25_7b</span></code> - 7B parameters
- <code class="docutils literal notranslate"><span class="pre">qwen25_14b</span></code> - 14B parameters
- <code class="docutils literal notranslate"><span class="pre">qwen25_32b</span></code> - 32B parameters</p>
<p><strong>Qwen 3 Series:</strong>
- <code class="docutils literal notranslate"><span class="pre">qwen3_0.6b</span></code> - 0.6B parameters
- <code class="docutils literal notranslate"><span class="pre">qwen3_1.7b</span></code> - 1.7B parameters
- <code class="docutils literal notranslate"><span class="pre">qwen3_4b</span></code> - 4B parameters
- <code class="docutils literal notranslate"><span class="pre">qwen3_8b</span></code> - 8B parameters
- <code class="docutils literal notranslate"><span class="pre">qwen3_14b</span></code> - 14B parameters
- <code class="docutils literal notranslate"><span class="pre">qwen3_32b</span></code> - 32B parameters</p>
</section>
<section id="model-configuration">
<h2>Model Configuration<a class="headerlink" href="#model-configuration" title="Link to this heading">¶</a></h2>
<section id="qwenconfig">
<h3>QwenConfig<a class="headerlink" href="#qwenconfig" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">fairseq2.models.qwen.</span></span><span class="sig-name descname"><span class="pre">QwenConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3584</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">152064</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tied_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'bool'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">28</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attn_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">28</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_key_value_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int</span> <span class="pre">|</span> <span class="pre">None'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qkv_proj_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'bool'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_norm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'bool'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k_norm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'bool'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_inner_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'int'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">18944</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rope_theta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'float'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'float'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fairseq2/models/qwen/config.html#QwenConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Configuration class for Qwen models. Defines the architecture parameters such as
model dimensions, number of layers, attention heads, and other architectural choices.</p>
<p><strong>Key Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_dim</span></code> - The dimensionality of the model (default: 3584)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_layers</span></code> - Number of decoder layers (default: 28)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_attn_heads</span></code> - Number of attention heads (default: 28)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_key_value_heads</span></code> - Number of key/value heads for GQA (default: 4)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> - Maximum sequence length (default: 32,768)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> - Vocabulary size (default: 152,064)</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">QwenConfig</span>

<span class="c1"># Create custom configuration</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">QwenConfig</span><span class="p">()</span>
<span class="n">config</span><span class="o">.</span><span class="n">model_dim</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">config</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">config</span><span class="o">.</span><span class="n">num_attn_heads</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">config</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="mi">16384</span>

<span class="c1"># Or get pre-defined architecture</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_qwen_model_hub</span>
<span class="n">hub</span> <span class="o">=</span> <span class="n">get_qwen_model_hub</span><span class="p">()</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">get_arch_config</span><span class="p">(</span><span class="s2">&quot;qwen25_7b&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.model_dim">
<span class="sig-name descname"><span class="pre">model_dim</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">3584</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.model_dim" title="Link to this definition">¶</a></dt>
<dd><p>The dimensionality of the model.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.max_seq_len">
<span class="sig-name descname"><span class="pre">max_seq_len</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">32768</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.max_seq_len" title="Link to this definition">¶</a></dt>
<dd><p>The maximum sequence length.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.vocab_size">
<span class="sig-name descname"><span class="pre">vocab_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">152064</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.vocab_size" title="Link to this definition">¶</a></dt>
<dd><p>The size of the vocabulary.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.tied_embeddings">
<span class="sig-name descname"><span class="pre">tied_embeddings</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.tied_embeddings" title="Link to this definition">¶</a></dt>
<dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, ties the embedding table and the output projection layer.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.num_layers">
<span class="sig-name descname"><span class="pre">num_layers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">28</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.num_layers" title="Link to this definition">¶</a></dt>
<dd><p>The number of decoder layers.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.num_attn_heads">
<span class="sig-name descname"><span class="pre">num_attn_heads</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">28</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.num_attn_heads" title="Link to this definition">¶</a></dt>
<dd><p>The number of attention heads in decoder layers.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.num_key_value_heads">
<span class="sig-name descname"><span class="pre">num_key_value_heads</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">4</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.num_key_value_heads" title="Link to this definition">¶</a></dt>
<dd><p>The number of key/value heads for Grouped Query Attention.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.head_dim">
<span class="sig-name descname"><span class="pre">head_dim</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.head_dim" title="Link to this definition">¶</a></dt>
<dd><p>The dimensionality of attention heads. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the standard
formula <code class="docutils literal notranslate"><span class="pre">model_dim</span> <span class="pre">//</span> <span class="pre">num_attn_heads</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.qkv_proj_bias">
<span class="sig-name descname"><span class="pre">qkv_proj_bias</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.qkv_proj_bias" title="Link to this definition">¶</a></dt>
<dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, query, key, and value projections learn an additive bias.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.q_norm">
<span class="sig-name descname"><span class="pre">q_norm</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.q_norm" title="Link to this definition">¶</a></dt>
<dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies Layer Normalization to projected attention queries.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.k_norm">
<span class="sig-name descname"><span class="pre">k_norm</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.k_norm" title="Link to this definition">¶</a></dt>
<dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, applies Layer Normalization to projected attention keys.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.ffn_inner_dim">
<span class="sig-name descname"><span class="pre">ffn_inner_dim</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">18944</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.ffn_inner_dim" title="Link to this definition">¶</a></dt>
<dd><p>The dimensionality of inner projection layers in feed-forward networks.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.rope_theta">
<span class="sig-name descname"><span class="pre">rope_theta</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1000000.0</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.rope_theta" title="Link to this definition">¶</a></dt>
<dd><p>The coefficient of the long-term decay of the Rotary position encoder.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenConfig.dropout_p">
<span class="sig-name descname"><span class="pre">dropout_p</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.0</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenConfig.dropout_p" title="Link to this definition">¶</a></dt>
<dd><p>The dropout probability on outputs of Transformer layers.</p>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="model-factory">
<h2>Model Factory<a class="headerlink" href="#model-factory" title="Link to this heading">¶</a></h2>
<section id="qwenfactory">
<h3>QwenFactory<a class="headerlink" href="#qwenfactory" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">fairseq2.models.qwen.</span></span><span class="sig-name descname"><span class="pre">QwenFactory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig" title="fairseq2.models.qwen.config.QwenConfig"><span class="pre">QwenConfig</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">gangs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="fairseq2.gang.html#fairseq2.gang.Gangs" title="fairseq2.gang.Gangs"><span class="pre">Gangs</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Factory class for creating Qwen models. Handles model instantiation and checkpoint loading.</p>
<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory.create_model">
<span class="sig-name descname"><span class="pre">create_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">TransformerLM</span></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory.create_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory.create_model" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory.create_embedding">
<span class="sig-name descname"><span class="pre">create_embedding</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="fairseq2.nn.embedding.html#fairseq2.nn.Embedding" title="fairseq2.nn.embedding.Embedding"><span class="pre">Embedding</span></a></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory.create_embedding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory.create_embedding" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory.create_decoder_frontend">
<span class="sig-name descname"><span class="pre">create_decoder_frontend</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="fairseq2.nn.embedding.html#fairseq2.nn.Embedding" title="fairseq2.nn.embedding.Embedding"><span class="pre">Embedding</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">TransformerFrontend</span></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory.create_decoder_frontend"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory.create_decoder_frontend" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory.create_decoder">
<span class="sig-name descname"><span class="pre">create_decoder</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">TransformerLMDecoder</span></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory.create_decoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory.create_decoder" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory.create_position_encoder">
<span class="sig-name descname"><span class="pre">create_position_encoder</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="fairseq2.nn.position_encoder.html#fairseq2.nn.PositionEncoder" title="fairseq2.nn.position_encoder.PositionEncoder"><span class="pre">PositionEncoder</span></a></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory.create_position_encoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory.create_position_encoder" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory.create_decoder_layer">
<span class="sig-name descname"><span class="pre">create_decoder_layer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="fairseq2.nn.position_encoder.html#fairseq2.nn.PositionEncoder" title="fairseq2.nn.position_encoder.PositionEncoder"><span class="pre">PositionEncoder</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">TransformerLMDecoderLayer</span></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory.create_decoder_layer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory.create_decoder_layer" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory.create_self_attention">
<span class="sig-name descname"><span class="pre">create_self_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="fairseq2.nn.position_encoder.html#fairseq2.nn.PositionEncoder" title="fairseq2.nn.position_encoder.PositionEncoder"><span class="pre">PositionEncoder</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">MultiheadAttention</span></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory.create_self_attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory.create_self_attention" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory.create_ffn">
<span class="sig-name descname"><span class="pre">create_ffn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">FeedForwardNetwork</span></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory.create_ffn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory.create_ffn" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory.create_final_projection">
<span class="sig-name descname"><span class="pre">create_final_projection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="fairseq2.nn.embedding.html#fairseq2.nn.Embedding" title="fairseq2.nn.embedding.Embedding"><span class="pre">Embedding</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="fairseq2.nn.projection.html#fairseq2.nn.Projection" title="fairseq2.nn.projection.Projection"><span class="pre">Projection</span></a></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory.create_final_projection"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory.create_final_projection" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory.create_layer_norm">
<span class="sig-name descname"><span class="pre">create_layer_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="fairseq2.nn.normalization.html#fairseq2.nn.LayerNorm" title="fairseq2.nn.normalization.LayerNorm"><span class="pre">LayerNorm</span></a></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory.create_layer_norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory.create_layer_norm" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenFactory.get_std_scale_factor">
<span class="sig-name descname"><span class="pre">get_std_scale_factor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#QwenFactory.get_std_scale_factor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenFactory.get_std_scale_factor" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="create-qwen-model">
<h3>create_qwen_model<a class="headerlink" href="#create-qwen-model" title="Link to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="fairseq2.models.qwen.create_qwen_model">
<span class="sig-prename descclassname"><span class="pre">fairseq2.models.qwen.</span></span><span class="sig-name descname"><span class="pre">create_qwen_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig" title="fairseq2.models.qwen.config.QwenConfig"><span class="pre">QwenConfig</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">TransformerLM</span></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/factory.html#create_qwen_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.create_qwen_model" title="Link to this definition">¶</a></dt>
<dd><p>Creates a Qwen model instance with the specified configuration.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_qwen_model</span><span class="p">,</span> <span class="n">QwenConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">QwenConfig</span><span class="p">()</span>
<span class="n">config</span><span class="o">.</span><span class="n">model_dim</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">config</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">24</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">create_qwen_model</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
<section id="tokenizer">
<h2>Tokenizer<a class="headerlink" href="#tokenizer" title="Link to this heading">¶</a></h2>
<section id="qwentokenizer">
<h3>QwenTokenizer<a class="headerlink" href="#qwentokenizer" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenTokenizer">
<em class="property"><span class="pre">final</span><span class="w"> </span><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">fairseq2.models.qwen.</span></span><span class="sig-name descname"><span class="pre">QwenTokenizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">HuggingFaceTokenModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fairseq2/models/qwen/tokenizer.html#QwenTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenTokenizer" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="fairseq2.data.tokenizers.html#fairseq2.data.tokenizers.Tokenizer" title="fairseq2.data.tokenizers.tokenizer.Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tokenizer</span></code></a></p>
<p>Tokenizer for Qwen models. Handles text encoding and decoding using the Qwen vocabulary.</p>
<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenTokenizer.create_encoder">
<span class="sig-name descname"><span class="pre">create_encoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="fairseq2.data.tokenizers.html#fairseq2.data.tokenizers.TokenEncoder" title="fairseq2.data.tokenizers.tokenizer.TokenEncoder"><span class="pre">TokenEncoder</span></a></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/tokenizer.html#QwenTokenizer.create_encoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenTokenizer.create_encoder" title="Link to this definition">¶</a></dt>
<dd><p>Constructs a token encoder.</p>
<p>The valid arguments for the <code class="docutils literal notranslate"><span class="pre">task</span></code>, <code class="docutils literal notranslate"><span class="pre">lang</span></code>, and <code class="docutils literal notranslate"><span class="pre">mode</span></code> parameters
are implementation specific. Refer to concrete <code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code>
subclasses for more information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>task</strong> – The task for which to generate token indices. Typically, <code class="docutils literal notranslate"><span class="pre">task</span></code> is
used to distinguish between different tasks such as ‘translation’ or
‘transcription’.</p></li>
<li><p><strong>lang</strong> – The language of generated token indices. Typically, multilingual
translation tasks use <code class="docutils literal notranslate"><span class="pre">lang</span></code> to distinguish between different
languages such as ‘en-US’ or ‘de-DE’.</p></li>
<li><p><strong>mode</strong> – The mode in which to generate token indices. Typically, translation
tasks use <code class="docutils literal notranslate"><span class="pre">mode</span></code> to distinguish between different modes such as
‘source’ or ‘target’.</p></li>
<li><p><strong>device</strong> – The device on which to construct tensors.</p></li>
<li><p><strong>pin_memory</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, uses pinned memory while constructing tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenTokenizer.create_raw_encoder">
<span class="sig-name descname"><span class="pre">create_raw_encoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="fairseq2.data.tokenizers.html#fairseq2.data.tokenizers.TokenEncoder" title="fairseq2.data.tokenizers.tokenizer.TokenEncoder"><span class="pre">TokenEncoder</span></a></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/tokenizer.html#QwenTokenizer.create_raw_encoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenTokenizer.create_raw_encoder" title="Link to this definition">¶</a></dt>
<dd><p>Constructs a raw token encoder with no control symbols.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> – The device on which to construct tensors.</p></li>
<li><p><strong>pin_memory</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, uses pinned memory for tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenTokenizer.create_decoder">
<span class="sig-name descname"><span class="pre">create_decoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_special_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="fairseq2.data.tokenizers.html#fairseq2.data.tokenizers.TokenDecoder" title="fairseq2.data.tokenizers.tokenizer.TokenDecoder"><span class="pre">TokenDecoder</span></a></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/tokenizer.html#QwenTokenizer.create_decoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenTokenizer.create_decoder" title="Link to this definition">¶</a></dt>
<dd><p>Constructs a token decoder.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenTokenizer.vocab_info">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vocab_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="fairseq2.data.tokenizers.html#fairseq2.data.tokenizers.VocabularyInfo" title="fairseq2.data.tokenizers.vocab_info.VocabularyInfo"><span class="pre">VocabularyInfo</span></a></em><a class="headerlink" href="#fairseq2.models.qwen.QwenTokenizer.vocab_info" title="Link to this definition">¶</a></dt>
<dd><p>The vocabulary information associated with the tokenizer.</p>
</dd></dl>

</dd></dl>

</section>
<section id="qwentokenizerconfig">
<h3>QwenTokenizerConfig<a class="headerlink" href="#qwentokenizerconfig" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenTokenizerConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">fairseq2.models.qwen.</span></span><span class="sig-name descname"><span class="pre">QwenTokenizerConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_im_end</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'bool'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/fairseq2/models/qwen/tokenizer.html#QwenTokenizerConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.QwenTokenizerConfig" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Configuration for the Qwen tokenizer.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QwenTokenizerConfig.use_im_end">
<span class="sig-name descname"><span class="pre">use_im_end</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#fairseq2.models.qwen.QwenTokenizerConfig.use_im_end" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="get-qwen-tokenizer-hub">
<h3>get_qwen_tokenizer_hub<a class="headerlink" href="#get-qwen-tokenizer-hub" title="Link to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="fairseq2.models.qwen.get_qwen_tokenizer_hub">
<span class="sig-prename descclassname"><span class="pre">fairseq2.models.qwen.</span></span><span class="sig-name descname"><span class="pre">get_qwen_tokenizer_hub</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="fairseq2.data.tokenizers.hub.html#fairseq2.data.tokenizers.hub.TokenizerHub" title="fairseq2.data.tokenizers.hub.TokenizerHub"><span class="pre">TokenizerHub</span></a><span class="p"><span class="pre">[</span></span><span class="pre">TokenizerT</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">TokenizerConfigT</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#fairseq2.models.qwen.get_qwen_tokenizer_hub" title="Link to this definition">¶</a></dt>
<dd><p>Returns the tokenizer hub for Qwen tokenizers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_qwen_tokenizer_hub</span>

<span class="n">tokenizer_hub</span> <span class="o">=</span> <span class="n">get_qwen_tokenizer_hub</span><span class="p">()</span>

<span class="c1"># Load tokenizer through hub</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer_hub</span><span class="o">.</span><span class="n">load_tokenizer</span><span class="p">(</span><span class="s2">&quot;qwen25_7b&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
<section id="interoperability">
<h2>Interoperability<a class="headerlink" href="#interoperability" title="Link to this heading">¶</a></h2>
<section id="convert-qwen-state-dict">
<h3>convert_qwen_state_dict<a class="headerlink" href="#convert-qwen-state-dict" title="Link to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="fairseq2.models.qwen.convert_qwen_state_dict">
<span class="sig-prename descclassname"><span class="pre">fairseq2.models.qwen.</span></span><span class="sig-name descname"><span class="pre">convert_qwen_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><span class="pre">object</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig" title="fairseq2.models.qwen.config.QwenConfig"><span class="pre">QwenConfig</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><span class="pre">object</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/interop.html#convert_qwen_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.convert_qwen_state_dict" title="Link to this definition">¶</a></dt>
<dd><p>Converts Qwen model state dictionaries between different formats (e.g., from Hugging Face format).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">convert_qwen_state_dict</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Load checkpoint from Hugging Face format</span>
<span class="n">hf_state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;qwen_hf_checkpoint.pt&quot;</span><span class="p">)</span>

<span class="c1"># Convert to fairseq2 format</span>
<span class="n">fs2_state_dict</span> <span class="o">=</span> <span class="n">convert_qwen_state_dict</span><span class="p">(</span><span class="n">hf_state_dict</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="export-qwen">
<h3>export_qwen<a class="headerlink" href="#export-qwen" title="Link to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="fairseq2.models.qwen.export_qwen">
<span class="sig-prename descclassname"><span class="pre">fairseq2.models.qwen.</span></span><span class="sig-name descname"><span class="pre">export_qwen</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><span class="pre">object</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig" title="fairseq2.models.qwen.config.QwenConfig"><span class="pre">QwenConfig</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">HuggingFaceExport</span></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/interop.html#export_qwen"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.export_qwen" title="Link to this definition">¶</a></dt>
<dd><p>Exports fairseq2 Qwen models to other formats for interoperability.</p>
</dd></dl>

</section>
</section>
<section id="sharding">
<h2>Sharding<a class="headerlink" href="#sharding" title="Link to this heading">¶</a></h2>
<section id="get-qwen-shard-specs">
<h3>get_qwen_shard_specs<a class="headerlink" href="#get-qwen-shard-specs" title="Link to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="fairseq2.models.qwen.get_qwen_shard_specs">
<span class="sig-prename descclassname"><span class="pre">fairseq2.models.qwen.</span></span><span class="sig-name descname"><span class="pre">get_qwen_shard_specs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig" title="fairseq2.models.qwen.config.QwenConfig"><span class="pre">QwenConfig</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ShardSpec</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/fairseq2/models/qwen/sharder.html#get_qwen_shard_specs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fairseq2.models.qwen.get_qwen_shard_specs" title="Link to this definition">¶</a></dt>
<dd><p>Returns sharding specifications for distributed training and inference of Qwen models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_qwen_shard_specs</span><span class="p">,</span> <span class="n">QwenConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">QwenConfig</span><span class="p">()</span>
<span class="n">shard_specs</span> <span class="o">=</span> <span class="n">get_qwen_shard_specs</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
<section id="constants">
<h2>Constants<a class="headerlink" href="#constants" title="Link to this heading">¶</a></h2>
<section id="qwen-family">
<h3>QWEN_FAMILY<a class="headerlink" href="#qwen-family" title="Link to this heading">¶</a></h3>
<dl class="py data">
<dt class="sig sig-object py" id="fairseq2.models.qwen.QWEN_FAMILY">
<span class="sig-prename descclassname"><span class="pre">fairseq2.models.qwen.</span></span><span class="sig-name descname"><span class="pre">QWEN_FAMILY</span></span><em class="property"><span class="w"> </span><span class="pre">=</span> <span class="pre">&quot;qwen&quot;</span></em><a class="headerlink" href="#fairseq2.models.qwen.QWEN_FAMILY" title="Link to this definition">¶</a></dt>
<dd><p>str(object=’’) -&gt; str
str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p>
<p>Create a new string object from the given object. If encoding or
errors is specified, then the object must expose a data buffer
that will be decoded using the given encoding and error handler.
Otherwise, returns the result of object.__str__() (if defined)
or repr(object).
encoding defaults to sys.getdefaultencoding().
errors defaults to ‘strict’.</p>
<p>The family name identifier for Qwen models.</p>
</dd></dl>

</section>
</section>
<section id="complete-examples">
<h2>Complete Examples<a class="headerlink" href="#complete-examples" title="Link to this heading">¶</a></h2>
<section id="basic-model-usage">
<h3>Basic Model Usage<a class="headerlink" href="#basic-model-usage" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_qwen_model_hub</span><span class="p">,</span> <span class="n">get_qwen_tokenizer_hub</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.device</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_default_device</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">BatchLayout</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">get_default_device</span><span class="p">()</span>

<span class="c1"># Load model and tokenizer</span>
<span class="n">hub</span> <span class="o">=</span> <span class="n">get_qwen_model_hub</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;qwen25_7b&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_qwen_tokenizer_hub</span><span class="p">()</span><span class="o">.</span><span class="n">load_tokenizer</span><span class="p">(</span><span class="s2">&quot;qwen25_7b&quot;</span><span class="p">)</span>

<span class="c1"># Prepare input</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;The capital of France is&quot;</span><span class="p">,</span> <span class="s2">&quot;The capital of Germany is&quot;</span><span class="p">]</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">create_encoder</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">encoder</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Run inference (simplified)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">seqs_layout</span> <span class="o">=</span> <span class="n">BatchLayout</span><span class="o">.</span><span class="n">of</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">seqs_layout</span><span class="o">=</span><span class="n">seqs_layout</span><span class="p">)</span>
    <span class="c1"># Process output...</span>
</pre></div>
</div>
</section>
<section id="custom-architecture">
<h3>Custom Architecture<a class="headerlink" href="#custom-architecture" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_qwen_model_hub</span><span class="p">,</span> <span class="n">QwenConfig</span>

<span class="n">hub</span> <span class="o">=</span> <span class="n">get_qwen_model_hub</span><span class="p">()</span>

<span class="c1"># Get base configuration and modify</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">get_arch_config</span><span class="p">(</span><span class="s2">&quot;qwen25_7b&quot;</span><span class="p">)</span>
<span class="n">config</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="mi">16384</span>  <span class="c1"># Reduce sequence length</span>
<span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span> <span class="o">=</span> <span class="mf">0.1</span>      <span class="c1"># Add dropout</span>

<span class="c1"># Create model with custom config</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">create_new_model</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="loading-from-custom-checkpoint">
<h3>Loading from Custom Checkpoint<a class="headerlink" href="#loading-from-custom-checkpoint" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_qwen_model_hub</span>

<span class="n">hub</span> <span class="o">=</span> <span class="n">get_qwen_model_hub</span><span class="p">()</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">get_arch_config</span><span class="p">(</span><span class="s2">&quot;qwen25_7b&quot;</span><span class="p">)</span>

<span class="c1"># Load from custom checkpoint</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;/path/to/my/qwen_checkpoint.pt&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">load_custom_model</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="architecture-comparison">
<h3>Architecture Comparison<a class="headerlink" href="#architecture-comparison" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">fairseq2.models.qwen</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_qwen_model_hub</span>

<span class="n">hub</span> <span class="o">=</span> <span class="n">get_qwen_model_hub</span><span class="p">()</span>

<span class="c1"># Compare different Qwen architectures</span>
<span class="n">architectures</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;qwen25_3b&quot;</span><span class="p">,</span> <span class="s2">&quot;qwen25_7b&quot;</span><span class="p">,</span> <span class="s2">&quot;qwen25_14b&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">arch</span> <span class="ow">in</span> <span class="n">architectures</span><span class="p">:</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">get_arch_config</span><span class="p">(</span><span class="n">arch</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">model_dim</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attn_heads</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">arch</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Model dim: </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">model_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Layers: </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Attention heads: </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">num_attn_heads</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Approx parameters: ~</span><span class="si">{</span><span class="n">params</span><span class="o">//</span><span class="mi">1_000_000</span><span class="si">}</span><span class="s2">M&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="see-also">
<h2>See Also<a class="headerlink" href="#see-also" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="fairseq2.models.hub.html"><span class="doc">fairseq2.models.hub</span></a> - Model hub API reference</p></li>
<li><p><a class="reference internal" href="../guides/add_model.html"><span class="doc"> Add Your Own Model</span></a> - Tutorial on adding new models</p></li>
<li><p><a class="reference internal" href="../basics/assets.html"><span class="doc"> Assets</span></a> - Understanding the asset system</p></li>
</ul>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="fairseq2.recipe.composition.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">fairseq2.recipe.composition</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="fairseq2.models.llama.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">fairseq2.models.llama</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/facebookresearch/fairseq2" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">fairseq2.models.qwen</a><ul>
<li><a class="reference internal" href="#quick-start">Quick Start</a></li>
<li><a class="reference internal" href="#available-models">Available Models</a></li>
<li><a class="reference internal" href="#model-configuration">Model Configuration</a><ul>
<li><a class="reference internal" href="#qwenconfig">QwenConfig</a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig"><code class="docutils literal notranslate"><span class="pre">QwenConfig</span></code></a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.model_dim"><code class="docutils literal notranslate"><span class="pre">QwenConfig.model_dim</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.max_seq_len"><code class="docutils literal notranslate"><span class="pre">QwenConfig.max_seq_len</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.vocab_size"><code class="docutils literal notranslate"><span class="pre">QwenConfig.vocab_size</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.tied_embeddings"><code class="docutils literal notranslate"><span class="pre">QwenConfig.tied_embeddings</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.num_layers"><code class="docutils literal notranslate"><span class="pre">QwenConfig.num_layers</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.num_attn_heads"><code class="docutils literal notranslate"><span class="pre">QwenConfig.num_attn_heads</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.num_key_value_heads"><code class="docutils literal notranslate"><span class="pre">QwenConfig.num_key_value_heads</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.head_dim"><code class="docutils literal notranslate"><span class="pre">QwenConfig.head_dim</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.qkv_proj_bias"><code class="docutils literal notranslate"><span class="pre">QwenConfig.qkv_proj_bias</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.q_norm"><code class="docutils literal notranslate"><span class="pre">QwenConfig.q_norm</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.k_norm"><code class="docutils literal notranslate"><span class="pre">QwenConfig.k_norm</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.ffn_inner_dim"><code class="docutils literal notranslate"><span class="pre">QwenConfig.ffn_inner_dim</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.rope_theta"><code class="docutils literal notranslate"><span class="pre">QwenConfig.rope_theta</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenConfig.dropout_p"><code class="docutils literal notranslate"><span class="pre">QwenConfig.dropout_p</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#model-factory">Model Factory</a><ul>
<li><a class="reference internal" href="#qwenfactory">QwenFactory</a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory"><code class="docutils literal notranslate"><span class="pre">QwenFactory</span></code></a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory.create_model"><code class="docutils literal notranslate"><span class="pre">QwenFactory.create_model()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory.create_embedding"><code class="docutils literal notranslate"><span class="pre">QwenFactory.create_embedding()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory.create_decoder_frontend"><code class="docutils literal notranslate"><span class="pre">QwenFactory.create_decoder_frontend()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory.create_decoder"><code class="docutils literal notranslate"><span class="pre">QwenFactory.create_decoder()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory.create_position_encoder"><code class="docutils literal notranslate"><span class="pre">QwenFactory.create_position_encoder()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory.create_decoder_layer"><code class="docutils literal notranslate"><span class="pre">QwenFactory.create_decoder_layer()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory.create_self_attention"><code class="docutils literal notranslate"><span class="pre">QwenFactory.create_self_attention()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory.create_ffn"><code class="docutils literal notranslate"><span class="pre">QwenFactory.create_ffn()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory.create_final_projection"><code class="docutils literal notranslate"><span class="pre">QwenFactory.create_final_projection()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory.create_layer_norm"><code class="docutils literal notranslate"><span class="pre">QwenFactory.create_layer_norm()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenFactory.get_std_scale_factor"><code class="docutils literal notranslate"><span class="pre">QwenFactory.get_std_scale_factor()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#create-qwen-model">create_qwen_model</a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.create_qwen_model"><code class="docutils literal notranslate"><span class="pre">create_qwen_model()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#tokenizer">Tokenizer</a><ul>
<li><a class="reference internal" href="#qwentokenizer">QwenTokenizer</a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenTokenizer"><code class="docutils literal notranslate"><span class="pre">QwenTokenizer</span></code></a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenTokenizer.create_encoder"><code class="docutils literal notranslate"><span class="pre">QwenTokenizer.create_encoder()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenTokenizer.create_raw_encoder"><code class="docutils literal notranslate"><span class="pre">QwenTokenizer.create_raw_encoder()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenTokenizer.create_decoder"><code class="docutils literal notranslate"><span class="pre">QwenTokenizer.create_decoder()</span></code></a></li>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenTokenizer.vocab_info"><code class="docutils literal notranslate"><span class="pre">QwenTokenizer.vocab_info</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#qwentokenizerconfig">QwenTokenizerConfig</a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenTokenizerConfig"><code class="docutils literal notranslate"><span class="pre">QwenTokenizerConfig</span></code></a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.QwenTokenizerConfig.use_im_end"><code class="docutils literal notranslate"><span class="pre">QwenTokenizerConfig.use_im_end</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#get-qwen-tokenizer-hub">get_qwen_tokenizer_hub</a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.get_qwen_tokenizer_hub"><code class="docutils literal notranslate"><span class="pre">get_qwen_tokenizer_hub()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#interoperability">Interoperability</a><ul>
<li><a class="reference internal" href="#convert-qwen-state-dict">convert_qwen_state_dict</a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.convert_qwen_state_dict"><code class="docutils literal notranslate"><span class="pre">convert_qwen_state_dict()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#export-qwen">export_qwen</a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.export_qwen"><code class="docutils literal notranslate"><span class="pre">export_qwen()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#sharding">Sharding</a><ul>
<li><a class="reference internal" href="#get-qwen-shard-specs">get_qwen_shard_specs</a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.get_qwen_shard_specs"><code class="docutils literal notranslate"><span class="pre">get_qwen_shard_specs()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#constants">Constants</a><ul>
<li><a class="reference internal" href="#qwen-family">QWEN_FAMILY</a><ul>
<li><a class="reference internal" href="#fairseq2.models.qwen.QWEN_FAMILY"><code class="docutils literal notranslate"><span class="pre">QWEN_FAMILY</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#complete-examples">Complete Examples</a><ul>
<li><a class="reference internal" href="#basic-model-usage">Basic Model Usage</a></li>
<li><a class="reference internal" href="#custom-architecture">Custom Architecture</a></li>
<li><a class="reference internal" href="#loading-from-custom-checkpoint">Loading from Custom Checkpoint</a></li>
<li><a class="reference internal" href="#architecture-comparison">Architecture Comparison</a></li>
</ul>
</li>
<li><a class="reference internal" href="#see-also">See Also</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=a133ad0e"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    </body>
</html>